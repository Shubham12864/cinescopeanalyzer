import praw
import prawcore
import asyncio
import os
import re
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import json
from collections import Counter, defaultdict
import numpy as np
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class EnhancedRedditAnalyzer:
    """
    Enhanced Reddit analyzer for comprehensive movie discussion analysis.
    """
    
    def __init__(self):
        """Initialize Reddit API connection"""
        self.reddit_client_id = os.getenv('REDDIT_CLIENT_ID')
        self.reddit_client_secret = os.getenv('REDDIT_CLIENT_SECRET')
        self.reddit_user_agent = os.getenv('REDDIT_USER_AGENT', 'CineScopeAnalyzer/2.0 (Enhanced Movie Analysis)')
        
        # Check if Reddit credentials are available and valid
        self.reddit_available = bool(
            self.reddit_client_id and 
            self.reddit_client_secret and 
            self.reddit_client_id != 'your_reddit_client_id' and
            self.reddit_client_secret != 'your_reddit_client_secret' and
            len(self.reddit_client_id) > 10 and
            len(self.reddit_client_secret) > 10
        )
        
        if not self.reddit_available:
            print("⚠️  Reddit API credentials not configured. Reddit analysis will be disabled.")
            print("   Set REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET to enable Reddit analysis.")
            self.reddit = None
            return
        
        print(f"🔍 Debug - Client ID: {self.reddit_client_id}")
        print(f"🔍 Debug - Client Secret: {self.reddit_client_secret[:10]}..." if self.reddit_client_secret else "None")
        print(f"🔍 Debug - Reddit available: {self.reddit_available}")
        
        if self.reddit_available:
            try:
                self.reddit = praw.Reddit(
                    client_id=self.reddit_client_id,
                    client_secret=self.reddit_client_secret,
                    user_agent=self.reddit_user_agent
                )
                # Test the connection with a simple subreddit access
                test_subreddit = self.reddit.subreddit('movies')
                posts = list(test_subreddit.hot(limit=1))  # Try to fetch one post
                print(f"✅ Reddit API connection established successfully! Found {len(posts)} test posts")
            except Exception as e:
                print(f"⚠️ Reddit API connection failed: {e}")
                self.reddit_available = False
                self.reddit = None
        else:
            print("⚠️ Reddit API credentials not configured. Using demo mode.")
            self.reddit = None
        
        # Enhanced subreddit list with categories
        self.subreddits = {
            'general_movies': ['movies', 'film', 'flicks'],
            'genre_specific': ['horror', 'scifi'],
            'discussion_focused': ['TrueFilm'],
        }
    
    async def comprehensive_movie_analysis(self, movie_title: str, imdb_id: str = None, 
                                         year: int = None, limit_per_subreddit: int = 50) -> Dict:
        """
        Perform comprehensive Reddit analysis for a movie
        """
        
        # If Reddit API is not available, return demo data
        if not self.reddit_available or not self.reddit:
            print(f"⚠️ Reddit API not available, returning demo analysis for '{movie_title}'")
            return self._generate_demo_analysis(movie_title, imdb_id, year)
        
        # For real Reddit API implementation
        print(f"🔍 Starting REAL Reddit analysis for '{movie_title}'...")
        
        try:
            # Search for movie discussions
            all_posts = []
            subreddit_stats = {}
            
            for category, subreddit_list in self.subreddits.items():
                for subreddit_name in subreddit_list[:2]:  # Limit to 2 subreddits for testing
                    try:
                        subreddit = self.reddit.subreddit(subreddit_name)
                        
                        # Search for the movie
                        search_query = f'"{movie_title}"' if ' ' in movie_title else movie_title
                        search_results = list(subreddit.search(search_query, limit=10, sort='relevance'))
                        
                        print(f"  - Found {len(search_results)} posts in r/{subreddit_name}")
                        
                        for post in search_results:
                            post_data = {
                                'id': post.id,
                                'subreddit': subreddit_name,
                                'title': post.title,
                                'selftext': getattr(post, 'selftext', ''),
                                'score': getattr(post, 'score', 0),
                                'num_comments': getattr(post, 'num_comments', 0),
                                'author': str(post.author) if hasattr(post, 'author') and post.author else '[deleted]',
                                'upvote_ratio': getattr(post, 'upvote_ratio', 0.5),
                                'created_utc': datetime.fromtimestamp(post.created_utc),
                                'url': post.url,
                                'permalink': f"https://reddit.com{post.permalink}",
                            }
                            all_posts.append(post_data)
                            
                    except Exception as e:
                        print(f"    ⚠️ Error searching r/{subreddit_name}: {e}")
                        continue
            
            print(f"✅ Collected {len(all_posts)} real Reddit posts!")
            
            # Perform analysis on real data
            return self._analyze_real_posts(movie_title, imdb_id, year, all_posts)
            
        except Exception as e:
            print(f"❌ Error in real Reddit analysis: {e}")
            print(f"🔄 Falling back to demo analysis for '{movie_title}'")
            return self._generate_demo_analysis(movie_title, imdb_id, year)
    
    def _analyze_real_posts(self, movie_title: str, imdb_id: str, year: int, posts: List[Dict]) -> Dict:
        """Analyze real Reddit posts"""
        import random
        
        total_posts = len(posts)
        if total_posts == 0:
            return self._generate_demo_analysis(movie_title, imdb_id, year)
        
        # Calculate real sentiment (simplified)
        sentiments = []
        for post in posts:
            # Simple sentiment based on score
            if post['score'] > 20:
                sentiment = 0.6
            elif post['score'] > 5:
                sentiment = 0.3
            elif post['score'] < -5:
                sentiment = -0.4
            else:
                sentiment = 0.1
            sentiments.append(sentiment)
        
        avg_sentiment = sum(sentiments) / len(sentiments) if sentiments else 0
        
        # Create sentiment distribution
        very_positive = sum(1 for s in sentiments if s > 0.5)
        positive = sum(1 for s in sentiments if 0.1 < s <= 0.5)
        neutral = sum(1 for s in sentiments if -0.1 <= s <= 0.1)
        negative = sum(1 for s in sentiments if -0.5 <= s < -0.1)
        very_negative = sum(1 for s in sentiments if s < -0.5)
        
        # Extract keywords from titles and text
        all_text = ' '.join([post['title'] + ' ' + post['selftext'] for post in posts]).lower()
        words = re.findall(r'\b\w{4,}\b', all_text)
        word_counts = Counter(words)
        top_keywords = [(word, count) for word, count in word_counts.most_common(10)]
        
        analysis_results = {
            'movie_info': {
                'title': movie_title,
                'imdb_id': imdb_id,
                'year': year,
                'analysis_timestamp': datetime.now().isoformat()
            },
            'collection_summary': {
                'total_posts': total_posts,
                'total_subreddits': len(set(p['subreddit'] for p in posts)),
                'date_range': {
                    'earliest': min(p['created_utc'] for p in posts).isoformat(),
                    'latest': max(p['created_utc'] for p in posts).isoformat(),
                    'span_days': (max(p['created_utc'] for p in posts) - min(p['created_utc'] for p in posts)).days
                },
                'search_queries_used': [movie_title]
            },
            'sentiment_analysis': {
                'overall_sentiment': {
                    'mean': avg_sentiment,
                    'median': avg_sentiment,
                    'std': 0.3,
                    'min': min(sentiments) if sentiments else 0,
                    'max': max(sentiments) if sentiments else 0
                },
                'distribution': {
                    'very_positive': very_positive,
                    'positive': positive,
                    'neutral': neutral,
                    'negative': negative,
                    'very_negative': very_negative
                },
                'percentiles': {
                    '10th': avg_sentiment - 0.3,
                    '25th': avg_sentiment - 0.15,
                    '75th': avg_sentiment + 0.15,
                    '90th': avg_sentiment + 0.3
                },
                'posts_vs_comments': {
                    'posts_mean_sentiment': avg_sentiment,
                    'comments_mean_sentiment': avg_sentiment,
                    'posts_count': total_posts,
                    'comments_count': sum(p['num_comments'] for p in posts)
                }
            },
            'community_insights': {
                'total_subreddits_analyzed': len(set(p['subreddit'] for p in posts)),
                'active_subreddits': len(set(p['subreddit'] for p in posts)),
                'engagement_metrics': {
                    'avg_posts_per_subreddit': total_posts / len(set(p['subreddit'] for p in posts)),
                    'avg_score_per_post': sum(p['score'] for p in posts) / total_posts,
                    'avg_comments_per_post': sum(p['num_comments'] for p in posts) / total_posts
                }
            },
            'temporal_analysis': {
                'date_range': {
                    'earliest': min(p['created_utc'] for p in posts).isoformat(),
                    'latest': max(p['created_utc'] for p in posts).isoformat(),
                    'total_days': (max(p['created_utc'] for p in posts) - min(p['created_utc'] for p in posts)).days
                },
                'peak_discussion_periods': [
                    {
                        'date': max(p['created_utc'] for p in posts).strftime('%Y-%m-%d'),
                        'rank': 1,
                        'post_count': total_posts,
                        'avg_sentiment': avg_sentiment
                    }
                ]
            },
            'content_analysis': {
                'text_statistics': {
                    'total_words': len(words),
                    'unique_words': len(word_counts),
                    'avg_post_length': len(all_text) / total_posts
                },
                'keyword_analysis': {
                    'top_keywords': top_keywords,
                    'total_unique_keywords': len(word_counts),
                    'keyword_diversity': len(word_counts) / len(words) if words else 0
                }
            },
            'discussion_themes': {
                'popular_themes': [
                    {'theme': 'discussion', 'count': total_posts, 'percentage': 100.0}
                ],
                'theme_diversity': 1
            },
            'user_behavior_analysis': {
                'total_unique_users': total_posts,  # Simplified
                'engagement_distribution': {
                    'avg_posts_per_user': 1.0
                }
            },
            'raw_data_summary': {
                'total_unique_users': total_posts,
                'total_text_length': len(all_text)
            }
        }
        
        return analysis_results
    
    def _generate_demo_analysis(self, movie_title: str, imdb_id: str = None, year: int = None) -> Dict:
        """Generate demo analysis when Reddit API is not available"""
        print(f"⚠️ Generating demo Reddit analysis for '{movie_title}' (Reddit API not configured)")
        
        import random
        
        # Generate realistic but fake data
        base_sentiment = random.uniform(-0.3, 0.7)  # Slightly positive bias
        total_posts = random.randint(25, 150)
        
        # Generate sentiment distribution
        very_positive = max(0, int(total_posts * random.uniform(0.1, 0.4)))
        positive = max(0, int(total_posts * random.uniform(0.15, 0.35)))
        neutral = max(0, int(total_posts * random.uniform(0.1, 0.3)))
        negative = max(0, int(total_posts * random.uniform(0.05, 0.25)))
        very_negative = max(0, total_posts - very_positive - positive - neutral - negative)
        
        # Ensure total adds up
        total_sentiment = very_positive + positive + neutral + negative + very_negative
        if total_sentiment != total_posts:
            neutral += (total_posts - total_sentiment)
        
        demo_analysis = {
            'movie_info': {
                'title': movie_title,
                'imdb_id': imdb_id,
                'year': year,
                'analysis_timestamp': datetime.now().isoformat()
            },
            'collection_summary': {
                'total_posts': total_posts,
                'total_subreddits': random.randint(8, 15),
                'date_range': {
                    'earliest': (datetime.now() - timedelta(days=random.randint(30, 365))).isoformat(),
                    'latest': datetime.now().isoformat(),
                    'span_days': random.randint(30, 365)
                },
                'search_queries_used': [movie_title, f'"{movie_title}"']
            },
            'sentiment_analysis': {
                'overall_sentiment': {
                    'mean': base_sentiment,
                    'median': base_sentiment + random.uniform(-0.1, 0.1),
                    'std': random.uniform(0.3, 0.8),
                    'min': -1.0,
                    'max': 1.0
                },
                'distribution': {
                    'very_positive': very_positive,
                    'positive': positive,
                    'neutral': neutral,
                    'negative': negative,
                    'very_negative': very_negative
                },
                'percentiles': {
                    '10th': base_sentiment - 0.4,
                    '25th': base_sentiment - 0.2,
                    '75th': base_sentiment + 0.2,
                    '90th': base_sentiment + 0.4
                },
                'posts_vs_comments': {
                    'posts_mean_sentiment': base_sentiment,
                    'comments_mean_sentiment': base_sentiment + random.uniform(-0.1, 0.1),
                    'posts_count': total_posts,
                    'comments_count': random.randint(total_posts * 2, total_posts * 8)
                }
            },
            'community_insights': {
                'total_subreddits_analyzed': random.randint(8, 15),
                'active_subreddits': random.randint(6, 12),
                'engagement_metrics': {
                    'avg_posts_per_subreddit': random.uniform(3, 15),
                    'avg_score_per_post': random.uniform(5, 50),
                    'avg_comments_per_post': random.uniform(2, 12)
                }
            },
            'temporal_analysis': {
                'date_range': {
                    'earliest': (datetime.now() - timedelta(days=random.randint(30, 365))).isoformat(),
                    'latest': datetime.now().isoformat(),
                    'total_days': random.randint(30, 365)
                },
                'peak_discussion_periods': [
                    {
                        'date': (datetime.now() - timedelta(days=random.randint(1, 30))).strftime('%Y-%m-%d'),
                        'rank': 1,
                        'post_count': random.randint(8, 25),
                        'avg_sentiment': base_sentiment + random.uniform(-0.2, 0.2)
                    }
                ]
            },
            'content_analysis': {
                'text_statistics': {
                    'total_words': random.randint(5000, 20000),
                    'unique_words': random.randint(1000, 5000),
                    'avg_post_length': random.uniform(50, 200)
                },
                'keyword_analysis': {
                    'top_keywords': [
                        ('acting', random.randint(5, 25)),
                        ('plot', random.randint(4, 20)),
                        ('cinematography', random.randint(3, 15)),
                        ('directing', random.randint(3, 12)),
                        ('soundtrack', random.randint(2, 10)),
                        ('effects', random.randint(2, 8)),
                        ('characters', random.randint(4, 18)),
                        ('story', random.randint(6, 22)),
                        ('performance', random.randint(3, 14)),
                        ('ending', random.randint(2, 12))
                    ],
                    'total_unique_keywords': random.randint(200, 800),
                    'keyword_diversity': random.uniform(0.3, 0.7)
                }
            },
            'discussion_themes': {
                'popular_themes': [
                    {'theme': 'review', 'count': random.randint(5, 20), 'percentage': random.uniform(15, 40)},
                    {'theme': 'discussion', 'count': random.randint(3, 15), 'percentage': random.uniform(10, 30)},
                    {'theme': 'question', 'count': random.randint(2, 10), 'percentage': random.uniform(5, 20)}
                ],
                'theme_diversity': random.randint(8, 15)
            },
            'user_behavior_analysis': {
                'total_unique_users': random.randint(total_posts // 2, total_posts),
                'engagement_distribution': {
                    'avg_posts_per_user': random.uniform(1.2, 2.5)
                }
            },
            'raw_data_summary': {
                'total_unique_users': random.randint(total_posts // 2, total_posts),
                'total_text_length': random.randint(10000, 50000)
            },
            'demo': True,  # Flag to indicate this is demo data
            'content_analysis': {
                'keyword_analysis': {
                    'top_keywords': [
                        ["movie", random.randint(8, 20)],
                        ["film", random.randint(6, 15)], 
                        ["great", random.randint(4, 12)],
                        ["good", random.randint(3, 10)],
                        ["story", random.randint(2, 8)],
                        ["acting", random.randint(2, 7)],
                        ["scenes", random.randint(1, 6)]
                    ]
                }
            }
        }
        
        return demo_analysis
